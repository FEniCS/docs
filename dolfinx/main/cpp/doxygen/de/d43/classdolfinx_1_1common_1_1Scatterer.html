<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.14.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>DOLFINx: Scatterer&lt; Container &gt; Class Template Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<script type="text/javascript" src="../../clipboard.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">DOLFINx<span id="projectnumber">&#160;0.11.0.0</span>
   </div>
   <div id="projectbrief">DOLFINx C++ interface</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.14.0 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('../../',true,false,'search.php','Search',false);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a href="../../d2/dc7/namespacedolfinx.html">dolfinx</a></li><li class="navelem"><a href="../../d7/de1/namespacedolfinx_1_1common.html">common</a></li><li class="navelem"><a href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html">Scatterer</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-types">Public Types</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="../../d8/df9/classdolfinx_1_1common_1_1Scatterer-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">Scatterer&lt; Container &gt; Class Template Reference</div></div>
</div><!--header-->
<div class="contents">

<p>A <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html" title="A Scatterer supports the scattering and gathering of distributed data that is associated with a commo...">Scatterer</a> supports the scattering and gathering of distributed data that is associated with a <a class="el" href="../../d2/d30/classdolfinx_1_1common_1_1IndexMap.html">common::IndexMap</a>, using <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a>.  
 <a href="#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="../../d4/d42/Scatterer_8h_source.html">Scatterer.h</a>&gt;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 id="header-pub-types" class="groupheader"><a id="pub-types" name="pub-types"></a>
Public Types</h2></td></tr>
<tr class="memitem:a9c2014b8eab41918aa55438cb370dd55" id="r_a9c2014b8eab41918aa55438cb370dd55"><td class="memItemLeft" align="right" valign="top"><a id="a9c2014b8eab41918aa55438cb370dd55" name="a9c2014b8eab41918aa55438cb370dd55"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><b>container_type</b> = Container</td></tr>
<tr class="memdesc:a9c2014b8eab41918aa55438cb370dd55"><td class="mdescLeft">&#160;</td><td class="mdescRight">Container type used to store local and remote indices. <br /></td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 id="header-pub-methods" class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aed4679805e627e96e439cc3b8b1c65e7" id="r_aed4679805e627e96e439cc3b8b1c65e7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aed4679805e627e96e439cc3b8b1c65e7">Scatterer</a> (const <a class="el" href="../../d2/d30/classdolfinx_1_1common_1_1IndexMap.html">IndexMap</a> &amp;map, int bs)</td></tr>
<tr class="memdesc:aed4679805e627e96e439cc3b8b1c65e7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Create a scatterer for data with a layout described by an <a class="el" href="../../d2/d30/classdolfinx_1_1common_1_1IndexMap.html">IndexMap</a> and a block size.  <br /></td></tr>
<tr class="memitem:af8ea99b293c8ea123a52db6d341476f9" id="r_af8ea99b293c8ea123a52db6d341476f9"><td class="memItemLeft" align="right" valign="top"><a id="af8ea99b293c8ea123a52db6d341476f9" name="af8ea99b293c8ea123a52db6d341476f9"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>Scatterer</b> (const <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html">Scatterer</a> &amp;scatterer)=default</td></tr>
<tr class="memdesc:af8ea99b293c8ea123a52db6d341476f9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Copy constructor. <br /></td></tr>
<tr class="memitem:af503fe66ca8c3e0151f1d9223fb5ef0e" id="r_af503fe66ca8c3e0151f1d9223fb5ef0e"><td class="memTemplParams" colspan="2">template&lt;class U&gt; </td></tr>
<tr class="memitem:af503fe66ca8c3e0151f1d9223fb5ef0e template"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af503fe66ca8c3e0151f1d9223fb5ef0e">Scatterer</a> (const <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html">Scatterer</a>&lt; U &gt; &amp;s)</td></tr>
<tr class="memdesc:af503fe66ca8c3e0151f1d9223fb5ef0e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Cast-copy constructor.  <br /></td></tr>
<tr class="memitem:a53234a50d2651d02a0684e31965bdba2" id="r_a53234a50d2651d02a0684e31965bdba2"><td class="memTemplParams" colspan="2">template&lt;typename T&gt; </td></tr>
<tr class="memitem:a53234a50d2651d02a0684e31965bdba2 template"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a53234a50d2651d02a0684e31965bdba2">scatter_fwd_begin</a> (const T *send_buffer, T *recv_buffer, MPI_Request &amp;request) const</td></tr>
<tr class="memdesc:a53234a50d2651d02a0684e31965bdba2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Start a non-blocking send of owned data to ranks that ghost the data using <em><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> neighbourhood collective communication</em> (recommended).  <br /></td></tr>
<tr class="memitem:a0858667656f7830e4bb568cce4bb294d" id="r_a0858667656f7830e4bb568cce4bb294d"><td class="memTemplParams" colspan="2">template&lt;typename T&gt; </td></tr>
<tr class="memitem:a0858667656f7830e4bb568cce4bb294d template"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a0858667656f7830e4bb568cce4bb294d">scatter_fwd_begin</a> (const T *send_buffer, T *recv_buffer, std::span&lt; MPI_Request &gt; requests) const</td></tr>
<tr class="memdesc:a0858667656f7830e4bb568cce4bb294d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Start a non-blocking send of owned data to ranks that ghost the data using <em>point-to-point <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communication</em>.  <br /></td></tr>
<tr class="memitem:a3bebfcef2134fb7646b0a5de20db9a6d" id="r_a3bebfcef2134fb7646b0a5de20db9a6d"><td class="memTemplParams" colspan="2">template&lt;typename T&gt; </td></tr>
<tr class="memitem:a3bebfcef2134fb7646b0a5de20db9a6d template"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a3bebfcef2134fb7646b0a5de20db9a6d">scatter_rev_begin</a> (const T *send_buffer, T *recv_buffer, MPI_Request &amp;request) const</td></tr>
<tr class="memdesc:a3bebfcef2134fb7646b0a5de20db9a6d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Start a non-blocking send of ghost data to ranks that own the data using <em><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> neighbourhood collective communication</em> (recommended).  <br /></td></tr>
<tr class="memitem:ad6b7e10a531d2a677ab68daa29d4f7fa" id="r_ad6b7e10a531d2a677ab68daa29d4f7fa"><td class="memTemplParams" colspan="2">template&lt;typename T&gt; </td></tr>
<tr class="memitem:ad6b7e10a531d2a677ab68daa29d4f7fa template"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ad6b7e10a531d2a677ab68daa29d4f7fa">scatter_rev_begin</a> (const T *send_buffer, T *recv_buffer, std::span&lt; MPI_Request &gt; requests) const</td></tr>
<tr class="memdesc:ad6b7e10a531d2a677ab68daa29d4f7fa"><td class="mdescLeft">&#160;</td><td class="mdescRight">Start a non-blocking send of ghost data to ranks that own the data using <em>point-to-point <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communication</em>.  <br /></td></tr>
<tr class="memitem:a22f24c30e82c48adcd61fa992c8406c6" id="r_a22f24c30e82c48adcd61fa992c8406c6"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a22f24c30e82c48adcd61fa992c8406c6">scatter_end</a> (std::span&lt; MPI_Request &gt; requests) const</td></tr>
<tr class="memdesc:a22f24c30e82c48adcd61fa992c8406c6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Complete non-blocking <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> point-to-point sends.  <br /></td></tr>
<tr class="memitem:a6bd29dc4f34d787f19915146ace238af" id="r_a6bd29dc4f34d787f19915146ace238af"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a6bd29dc4f34d787f19915146ace238af">scatter_end</a> (MPI_Request &amp;request) const</td></tr>
<tr class="memdesc:a6bd29dc4f34d787f19915146ace238af"><td class="mdescLeft">&#160;</td><td class="mdescRight">Complete a non-blocking <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> neighbourhood collective send.  <br /></td></tr>
<tr class="memitem:a569bd8eeb281f1a5191e6f0f12cfb9d6" id="r_a569bd8eeb281f1a5191e6f0f12cfb9d6"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="#a9c2014b8eab41918aa55438cb370dd55">container_type</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a569bd8eeb281f1a5191e6f0f12cfb9d6">local_indices</a> () const noexcept</td></tr>
<tr class="memdesc:a569bd8eeb281f1a5191e6f0f12cfb9d6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Array of indices for packing/unpacking owned data to/from a send/receive buffer.  <br /></td></tr>
<tr class="memitem:a61e39d73a4761cfa31d6ffadc1d552c2" id="r_a61e39d73a4761cfa31d6ffadc1d552c2"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="#a9c2014b8eab41918aa55438cb370dd55">container_type</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a61e39d73a4761cfa31d6ffadc1d552c2">remote_indices</a> () const noexcept</td></tr>
<tr class="memdesc:a61e39d73a4761cfa31d6ffadc1d552c2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Array of indices for packing/unpacking ghost data to/from a send/receive buffer.  <br /></td></tr>
<tr class="memitem:a347c440e68b4a4e881d1f7f2a23c94bc" id="r_a347c440e68b4a4e881d1f7f2a23c94bc"><td class="memItemLeft" align="right" valign="top">std::size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a347c440e68b4a4e881d1f7f2a23c94bc">num_p2p_requests</a> ()</td></tr>
<tr class="memdesc:a347c440e68b4a4e881d1f7f2a23c94bc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Number of required <span class="tt">MPI_Request</span>s for point-to-point communication.  <br /></td></tr>
</table>
<a name="details" id="details"></a><h2 id="header-details" class="groupheader">Detailed Description</h2>
<div class="textblock"><div class="compoundTemplParams">template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt;<br />
class dolfinx::common::Scatterer&lt; Container &gt;</div><p>A <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html" title="A Scatterer supports the scattering and gathering of distributed data that is associated with a commo...">Scatterer</a> supports the scattering and gathering of distributed data that is associated with a <a class="el" href="../../d2/d30/classdolfinx_1_1common_1_1IndexMap.html">common::IndexMap</a>, using <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a>. </p>
<p>Scatter and gather operations can use:</p><ol type="1">
<li><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> neighbourhood collectives (recommended), or</li>
<li>Non-blocking point-to-point communication modes.</li>
</ol>
<p>The implementation is designed for sparse communication patterns, as is typical of patterns based on an <a class="el" href="../../d2/d30/classdolfinx_1_1common_1_1IndexMap.html">IndexMap</a>.</p>
<p>A <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html" title="A Scatterer supports the scattering and gathering of distributed data that is associated with a commo...">Scatterer</a> is stateless, i.e. it provides the required information and static data for a given parallel communication pattern but does not provide any communication caches or track the status of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> requests. Callers of the a <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html" title="A Scatterer supports the scattering and gathering of distributed data that is associated with a commo...">Scatterer</a>'s members are responsible for managing buffer and <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> request handles.</p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">Container</td><td>Container type for storing the 'local' and 'remote' indices. On CPUs this is normally <span class="tt">std::vector&lt;std::int32_t&gt;</span>. For GPUs the container should store the indices on the device, e.g. using <span class="tt">thrust::device_vector&lt;std::int32_t&gt;</span>. </td></tr>
  </table>
  </dd>
</dl>
</div><a name="doc-constructors" id="doc-constructors"></a><h2 id="header-doc-constructors" class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="aed4679805e627e96e439cc3b8b1c65e7" name="aed4679805e627e96e439cc3b8b1c65e7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed4679805e627e96e439cc3b8b1c65e7">&#9670;&#160;</a></span>Scatterer() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html">Scatterer</a> </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="../../d2/d30/classdolfinx_1_1common_1_1IndexMap.html">IndexMap</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>map</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>bs</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Create a scatterer for data with a layout described by an <a class="el" href="../../d2/d30/classdolfinx_1_1common_1_1IndexMap.html">IndexMap</a> and a block size. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">map</td><td>Index map that describes the parallel layout of data. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">bs</td><td>Number of values associated with each <span class="tt">map</span> index (the block size). </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="af503fe66ca8c3e0151f1d9223fb5ef0e" name="af503fe66ca8c3e0151f1d9223fb5ef0e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af503fe66ca8c3e0151f1d9223fb5ef0e">&#9670;&#160;</a></span>Scatterer() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<div class="memtemplate">
template&lt;class U&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html">Scatterer</a> </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html">Scatterer</a>&lt; U &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>s</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Cast-copy constructor. </p>
<p>Create a copy of a <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html" title="A Scatterer supports the scattering and gathering of distributed data that is associated with a commo...">Scatterer</a>, were the copy uses a different storage container for indices that are used in <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communication. Example usage includes creating from a CPU-suitable <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html" title="A Scatterer supports the scattering and gathering of distributed data that is associated with a commo...">Scatterer</a> a GPU-suitable <a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html" title="A Scatterer supports the scattering and gathering of distributed data that is associated with a commo...">Scatterer</a> that can be used with GPU-aware <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> to move data between devices. This would be typical when copying a <a class="el" href="../../d2/d0b/classdolfinx_1_1la_1_1Vector.html" title="A vector that can be distributed across processes.">la::Vector</a> or <a class="el" href="../../dc/dfa/classdolfinx_1_1la_1_1MatrixCSR.html" title="Distributed sparse matrix using compressed sparse row storage.">la::MatrixCSR</a> to/from a GPU. When copying a vector or matrix to/from a GPU, the underlying Scatter that manages parallel communication will usually be copied too with a different storage container.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">s</td><td><a class="el" href="../../de/d43/classdolfinx_1_1common_1_1Scatterer.html" title="A Scatterer supports the scattering and gathering of distributed data that is associated with a commo...">Scatterer</a> to copy </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a name="doc-func-members" id="doc-func-members"></a><h2 id="header-doc-func-members" class="groupheader">Member Function Documentation</h2>
<a id="a569bd8eeb281f1a5191e6f0f12cfb9d6" name="a569bd8eeb281f1a5191e6f0f12cfb9d6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a569bd8eeb281f1a5191e6f0f12cfb9d6">&#9670;&#160;</a></span>local_indices()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">const <a class="el" href="#a9c2014b8eab41918aa55438cb370dd55">container_type</a> &amp; local_indices </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span><span class="mlabel noexcept">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Array of indices for packing/unpacking owned data to/from a send/receive buffer. </p>
<p>For a forward scatter, the indices are used to copy required entries in the owned part of the data array into the appropriate position in a send buffer. For a reverse scatter, indices are used for assigning (accumulating) the receive buffer values into correct position in the owned part of the data array.</p>
<p>For a forward scatter, if <span class="tt">x</span> is the owned part of an array and <span class="tt">send_buffer</span> is the send buffer, <span class="tt">send_buffer</span> is packed such that: </p><pre class="fragment">auto&amp; idx = scatterer.local_indices()
std::vector&lt;T&gt; send_buffer(idx.size())
for (std::size_t i = 0; i &lt; idx.size(); ++i)
    send_buffer[i] = x[idx[i]];
</pre><p>For a reverse scatter, if <span class="tt">recv_buffer</span> is the received buffer, then <span class="tt">x</span> is updated by </p><pre class="fragment">auto&amp; idx = scatterer.local_indices()
std::vector&lt;T&gt; recv_buffer(idx.size())
for (std::size_t i = 0; i &lt; idx.size(); ++i)
    x[idx[i]] = op(recv_buffer[i], x[idx[i]]);
</pre><p>where <span class="tt">op</span> is a binary operation, e.g. <span class="tt">x[idx[i]] = buffer[i]</span> or <span class="tt">x[idx[i]] += buffer[i]</span>.</p>
<dl class="section return"><dt>Returns</dt><dd>Indices container. </dd></dl>

</div>
</div>
<a id="a347c440e68b4a4e881d1f7f2a23c94bc" name="a347c440e68b4a4e881d1f7f2a23c94bc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a347c440e68b4a4e881d1f7f2a23c94bc">&#9670;&#160;</a></span>num_p2p_requests()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::size_t num_p2p_requests </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Number of required <span class="tt">MPI_Request</span>s for point-to-point communication. </p>
<dl class="section return"><dt>Returns</dt><dd>Numer of required <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> request handles. </dd></dl>

</div>
</div>
<a id="a61e39d73a4761cfa31d6ffadc1d552c2" name="a61e39d73a4761cfa31d6ffadc1d552c2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a61e39d73a4761cfa31d6ffadc1d552c2">&#9670;&#160;</a></span>remote_indices()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">const <a class="el" href="#a9c2014b8eab41918aa55438cb370dd55">container_type</a> &amp; remote_indices </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span><span class="mlabel noexcept">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Array of indices for packing/unpacking ghost data to/from a send/receive buffer. </p>
<p>For a forward scatter, the indices are to copy required entries in the owned array into the appropriate position in a send buffer. For a reverse scatter, indices are used for assigning (accumulating) the receive buffer values to correct position in the owned array.</p>
<p>For a forward scatter, if <span class="tt">xg</span> is the ghost part of the data array and <span class="tt">recv_buffer</span> is the receive buffer, <span class="tt">xg</span> is updated that </p><pre class="fragment">auto&amp; idx = scatterer.remote_indices()
std::vector&lt;T&gt; recv_buffer(idx.size())
for (std::size_t i = 0; i &lt; idx.size(); ++i)
    xg[idx[i]] = recv_buffer[i];
</pre><p>For a reverse scatter, if <span class="tt">send_buffer</span> is the send buffer, then <span class="tt">send_buffer</span> is packaged such that: </p><pre class="fragment">auto&amp; idx = scatterer.local_indices()
std::vector&lt;T&gt; send_buffer(idx.size())
for (std::size_t i = 0; i &lt; idx.size(); ++i)
    send_buffer[i] = xg[idx[i]];
</pre><dl class="section return"><dt>Returns</dt><dd>Indices container. </dd></dl>

</div>
</div>
<a id="a6bd29dc4f34d787f19915146ace238af" name="a6bd29dc4f34d787f19915146ace238af"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6bd29dc4f34d787f19915146ace238af">&#9670;&#160;</a></span>scatter_end() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void scatter_end </td>
          <td>(</td>
          <td class="paramtype">MPI_Request &amp;</td>          <td class="paramname"><span class="paramname"><em>request</em></span></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Complete a non-blocking <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> neighbourhood collective send. </p>
<p>This function completes the communication started by <a class="el" href="#a53234a50d2651d02a0684e31965bdba2" title="Start a non-blocking send of owned data to ranks that ghost the data using MPI neighbourhood collecti...">scatter_fwd_begin</a> or <a class="el" href="#a3bebfcef2134fb7646b0a5de20db9a6d" title="Start a non-blocking send of ghost data to ranks that own the data using MPI neighbourhood collective...">scatter_rev_begin</a>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">request</td><td><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> request handle for tracking the status of the send. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a22f24c30e82c48adcd61fa992c8406c6" name="a22f24c30e82c48adcd61fa992c8406c6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a22f24c30e82c48adcd61fa992c8406c6">&#9670;&#160;</a></span>scatter_end() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void scatter_end </td>
          <td>(</td>
          <td class="paramtype">std::span&lt; MPI_Request &gt;</td>          <td class="paramname"><span class="paramname"><em>requests</em></span></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Complete non-blocking <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> point-to-point sends. </p>
<p>This function completes the communication started by <a class="el" href="#a53234a50d2651d02a0684e31965bdba2" title="Start a non-blocking send of owned data to ranks that ghost the data using MPI neighbourhood collecti...">scatter_fwd_begin</a> or <a class="el" href="#a3bebfcef2134fb7646b0a5de20db9a6d" title="Start a non-blocking send of ghost data to ranks that own the data using MPI neighbourhood collective...">scatter_rev_begin</a>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">requests</td><td><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> request handles for tracking the status of sends. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a53234a50d2651d02a0684e31965bdba2" name="a53234a50d2651d02a0684e31965bdba2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a53234a50d2651d02a0684e31965bdba2">&#9670;&#160;</a></span>scatter_fwd_begin() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<div class="memtemplate">
template&lt;typename T&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void scatter_fwd_begin </td>
          <td>(</td>
          <td class="paramtype">const T *</td>          <td class="paramname"><span class="paramname"><em>send_buffer</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *</td>          <td class="paramname"><span class="paramname"><em>recv_buffer</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MPI_Request &amp;</td>          <td class="paramname"><span class="paramname"><em>request</em></span>&#160;) const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Start a non-blocking send of owned data to ranks that ghost the data using <em><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> neighbourhood collective communication</em> (recommended). </p>
<p>The communication is completed by calling <a class="el" href="#a22f24c30e82c48adcd61fa992c8406c6" title="Complete non-blocking MPI point-to-point sends.">Scatterer::scatter_end</a>. See <a class="el" href="#a569bd8eeb281f1a5191e6f0f12cfb9d6" title="Array of indices for packing/unpacking owned data to/from a send/receive buffer.">local_indices</a> for instructions on packing <span class="tt">send_buffer</span> and <a class="el" href="#a61e39d73a4761cfa31d6ffadc1d552c2" title="Array of indices for packing/unpacking ghost data to/from a send/receive buffer.">remote_indices</a> for instructions on unpacking <span class="tt">recv_buffer</span>.</p>
<dl class="section note"><dt>Note</dt><dd>The send and receive buffers must <b>not</b> to be changed or accessed until after a call to <a class="el" href="#a22f24c30e82c48adcd61fa992c8406c6" title="Complete non-blocking MPI point-to-point sends.">Scatterer::scatter_end</a>.</dd>
<dd>
The pointers <span class="tt">send_buffer</span> and <span class="tt">recv_buffer</span> must be pointers to the data on the <em>target device</em>. E.g., if the send and receive buffers are allocated on a GPU, the <span class="tt">send_buffer</span> and <span class="tt">recv_buffer</span> should be device pointers.</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">send_buffer</td><td>Packed local data associated with each owned local index to be sent to processes where the data is ghosted. See <a class="el" href="#a569bd8eeb281f1a5191e6f0f12cfb9d6" title="Array of indices for packing/unpacking owned data to/from a send/receive buffer.">Scatterer::local_indices</a> for the order of the buffer and how to pack. </td></tr>
    <tr><td class="paramdir">[in,out]</td><td class="paramname">recv_buffer</td><td>Buffer for storing received data. See <a class="el" href="#a61e39d73a4761cfa31d6ffadc1d552c2" title="Array of indices for packing/unpacking ghost data to/from a send/receive buffer.">Scatterer::remote_indices</a> for the order of the buffer and how to unpack. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">request</td><td><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> request handle for tracking the status of the non-blocking communication. The same request handle should be passed to <a class="el" href="#a22f24c30e82c48adcd61fa992c8406c6" title="Complete non-blocking MPI point-to-point sends.">Scatterer::scatter_end</a> to complete the communication. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a0858667656f7830e4bb568cce4bb294d" name="a0858667656f7830e4bb568cce4bb294d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0858667656f7830e4bb568cce4bb294d">&#9670;&#160;</a></span>scatter_fwd_begin() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<div class="memtemplate">
template&lt;typename T&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void scatter_fwd_begin </td>
          <td>(</td>
          <td class="paramtype">const T *</td>          <td class="paramname"><span class="paramname"><em>send_buffer</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *</td>          <td class="paramname"><span class="paramname"><em>recv_buffer</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::span&lt; MPI_Request &gt;</td>          <td class="paramname"><span class="paramname"><em>requests</em></span>&#160;) const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Start a non-blocking send of owned data to ranks that ghost the data using <em>point-to-point <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communication</em>. </p>
<p>See <a class="el" href="#a53234a50d2651d02a0684e31965bdba2" title="Start a non-blocking send of owned data to ranks that ghost the data using MPI neighbourhood collecti...">scatter_fwd_begin</a> for a detailed explanation of usage, including on the send and receive buffer packing and unpacking</p>
<dl class="section note"><dt>Note</dt><dd>Use of the neighbourhood version of <a class="el" href="#a53234a50d2651d02a0684e31965bdba2" title="Start a non-blocking send of owned data to ranks that ghost the data using MPI neighbourhood collecti...">scatter_fwd_begin</a> is recommended over this version.</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">send_buffer</td><td>Send buffer. </td></tr>
    <tr><td class="paramdir">[in,out]</td><td class="paramname">recv_buffer</td><td>Receive buffer. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">requests</td><td>List of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> request handles. The length of the list must be <a class="el" href="#a347c440e68b4a4e881d1f7f2a23c94bc" title="Number of required MPI_Requests for point-to-point communication.">num_p2p_requests()</a> </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a3bebfcef2134fb7646b0a5de20db9a6d" name="a3bebfcef2134fb7646b0a5de20db9a6d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3bebfcef2134fb7646b0a5de20db9a6d">&#9670;&#160;</a></span>scatter_rev_begin() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<div class="memtemplate">
template&lt;typename T&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void scatter_rev_begin </td>
          <td>(</td>
          <td class="paramtype">const T *</td>          <td class="paramname"><span class="paramname"><em>send_buffer</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *</td>          <td class="paramname"><span class="paramname"><em>recv_buffer</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MPI_Request &amp;</td>          <td class="paramname"><span class="paramname"><em>request</em></span>&#160;) const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Start a non-blocking send of ghost data to ranks that own the data using <em><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> neighbourhood collective communication</em> (recommended). </p>
<p>The communication is completed by calling <a class="el" href="#a22f24c30e82c48adcd61fa992c8406c6" title="Complete non-blocking MPI point-to-point sends.">Scatterer::scatter_end</a>. See <a class="el" href="#a61e39d73a4761cfa31d6ffadc1d552c2" title="Array of indices for packing/unpacking ghost data to/from a send/receive buffer.">remote_indices</a> for instructions on packing <span class="tt">send_buffer</span> and <a class="el" href="#a569bd8eeb281f1a5191e6f0f12cfb9d6" title="Array of indices for packing/unpacking owned data to/from a send/receive buffer.">local_indices</a> for instructions on unpacking <span class="tt">recv_buffer</span>.</p>
<dl class="section note"><dt>Note</dt><dd>The send and receive buffers must <b>not</b> to be changed or accessed until after a call to <a class="el" href="#a22f24c30e82c48adcd61fa992c8406c6" title="Complete non-blocking MPI point-to-point sends.">Scatterer::scatter_end</a>.</dd>
<dd>
The pointers <span class="tt">send_buffer</span> and <span class="tt">recv_buffer</span> must be pointers to the data on the <em>target device</em>. E.g., if the send and receive buffers are allocated on a GPU, the <span class="tt">send_buffer</span> and <span class="tt">recv_buffer</span> should be device pointers.</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">send_buffer</td><td>Data associated with each ghost index. This data is sent to process that owns the index. See <a class="el" href="#a61e39d73a4761cfa31d6ffadc1d552c2" title="Array of indices for packing/unpacking ghost data to/from a send/receive buffer.">Scatterer::remote_indices</a> for the order of the buffer and how to pack. </td></tr>
    <tr><td class="paramdir">[in,out]</td><td class="paramname">recv_buffer</td><td>Buffer for storing received data. See <a class="el" href="#a569bd8eeb281f1a5191e6f0f12cfb9d6" title="Array of indices for packing/unpacking owned data to/from a send/receive buffer.">Scatterer::local_indices</a> for the order of the buffer and how to unpack. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">request</td><td><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> request handle for tracking the status of the non-blocking communication. The same request handle should be passed to <a class="el" href="#a22f24c30e82c48adcd61fa992c8406c6" title="Complete non-blocking MPI point-to-point sends.">Scatterer::scatter_end</a> to complete the communication. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ad6b7e10a531d2a677ab68daa29d4f7fa" name="ad6b7e10a531d2a677ab68daa29d4f7fa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad6b7e10a531d2a677ab68daa29d4f7fa">&#9670;&#160;</a></span>scatter_rev_begin() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Container = std::vector&lt;std::int32_t&gt;&gt; </div>
<div class="memtemplate">
template&lt;typename T&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void scatter_rev_begin </td>
          <td>(</td>
          <td class="paramtype">const T *</td>          <td class="paramname"><span class="paramname"><em>send_buffer</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *</td>          <td class="paramname"><span class="paramname"><em>recv_buffer</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::span&lt; MPI_Request &gt;</td>          <td class="paramname"><span class="paramname"><em>requests</em></span>&#160;) const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Start a non-blocking send of ghost data to ranks that own the data using <em>point-to-point <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communication</em>. </p>
<p>See <a class="el" href="#a3bebfcef2134fb7646b0a5de20db9a6d" title="Start a non-blocking send of ghost data to ranks that own the data using MPI neighbourhood collective...">scatter_rev_begin</a> for a detailed explanation of usage, including on the send and receive buffer packing and unpacking</p>
<dl class="section note"><dt>Note</dt><dd>Use of the neighbourhood version of <a class="el" href="#a3bebfcef2134fb7646b0a5de20db9a6d" title="Start a non-blocking send of ghost data to ranks that own the data using MPI neighbourhood collective...">scatter_rev_begin</a> is recommended over this version</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">send_buffer</td><td>Send buffer. </td></tr>
    <tr><td class="paramdir">[in,out]</td><td class="paramname">recv_buffer</td><td>Receive buffer. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">requests</td><td>List of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> request handles. The length of the list must be <a class="el" href="#a347c440e68b4a4e881d1f7f2a23c94bc" title="Number of required MPI_Requests for point-to-point communication.">num_p2p_requests()</a> </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>/__w/dolfinx/dolfinx/cpp/dolfinx/common/<a class="el" href="../../d4/d42/Scatterer_8h_source.html">Scatterer.h</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.14.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
