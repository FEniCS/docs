<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>DOLFINx: dolfinx::MPI Namespace Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">DOLFINx<span id="projectnumber">&#160;0.7.3</span>
   </div>
   <div id="projectbrief">DOLFINx C++ interface</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="../../d2/dc7/namespacedolfinx.html">dolfinx</a></li><li class="navelem"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html">MPI</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#enum-members">Enumerations</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">dolfinx::MPI Namespace Reference</div></div>
</div><!--header-->
<div class="contents">

<p><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> support functionality.  
<a href="../../da/dfe/namespacedolfinx_1_1MPI.html#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../dd/d1d/classdolfinx_1_1MPI_1_1Comm.html">Comm</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A duplicate <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communicator and manage lifetime of the communicator.  <a href="../../dd/d1d/classdolfinx_1_1MPI_1_1Comm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../df/ddb/structdolfinx_1_1MPI_1_1dependent__false.html">dependent_false</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="enum-members" name="enum-members"></a>
Enumerations</h2></td></tr>
<tr class="memitem:afcd86812aa931ca4c65bcccedc27592d" id="r_afcd86812aa931ca4c65bcccedc27592d"><td class="memItemLeft" align="right" valign="top"><a id="afcd86812aa931ca4c65bcccedc27592d" name="afcd86812aa931ca4c65bcccedc27592d"></a>enum class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#afcd86812aa931ca4c65bcccedc27592d">tag</a> : int { <b>consensus_pcx</b>
, <b>consensus_pex</b>
 }</td></tr>
<tr class="memdesc:afcd86812aa931ca4c65bcccedc27592d"><td class="mdescLeft">&#160;</td><td class="mdescRight">MPI communication tags. <br /></td></tr>
<tr class="separator:afcd86812aa931ca4c65bcccedc27592d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ab6a01b5be2487cb6674b70ce6a89bad0" id="r_ab6a01b5be2487cb6674b70ce6a89bad0"><td class="memItemLeft" align="right" valign="top"><a id="ab6a01b5be2487cb6674b70ce6a89bad0" name="ab6a01b5be2487cb6674b70ce6a89bad0"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><b>rank</b> (MPI_Comm comm)</td></tr>
<tr class="memdesc:ab6a01b5be2487cb6674b70ce6a89bad0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return process rank for the communicator. <br /></td></tr>
<tr class="separator:ab6a01b5be2487cb6674b70ce6a89bad0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa5036b153ca256f285b70638805fd6f3" id="r_aa5036b153ca256f285b70638805fd6f3"><td class="memItemLeft" align="right" valign="top"><a id="aa5036b153ca256f285b70638805fd6f3" name="aa5036b153ca256f285b70638805fd6f3"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><b>size</b> (MPI_Comm comm)</td></tr>
<tr class="memdesc:aa5036b153ca256f285b70638805fd6f3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return size of the group (number of processes) associated with the communicator. <br /></td></tr>
<tr class="separator:aa5036b153ca256f285b70638805fd6f3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9cc32591cea34ccd303e379a6d694c6e" id="r_a9cc32591cea34ccd303e379a6d694c6e"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#a9cc32591cea34ccd303e379a6d694c6e">check_error</a> (MPI_Comm comm, int code)</td></tr>
<tr class="memdesc:a9cc32591cea34ccd303e379a6d694c6e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Check <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> error code. If the error code is not equal to MPI_SUCCESS, then std::abort is called.  <br /></td></tr>
<tr class="separator:a9cc32591cea34ccd303e379a6d694c6e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acb5b18fba64080a1c968fb16f556def0" id="r_acb5b18fba64080a1c968fb16f556def0"><td class="memItemLeft" align="right" valign="top">constexpr std::array&lt; std::int64_t, 2 &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#acb5b18fba64080a1c968fb16f556def0">local_range</a> (int <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#ab6a01b5be2487cb6674b70ce6a89bad0">rank</a>, std::int64_t N, int <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#aa5036b153ca256f285b70638805fd6f3">size</a>)</td></tr>
<tr class="memdesc:acb5b18fba64080a1c968fb16f556def0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return local range for the calling process, partitioning the global [0, N - 1] range across all ranks into partitions of almost equal size.  <br /></td></tr>
<tr class="separator:acb5b18fba64080a1c968fb16f556def0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5dd58c219d3ac12735f200e442b9ccb0" id="r_a5dd58c219d3ac12735f200e442b9ccb0"><td class="memItemLeft" align="right" valign="top">constexpr int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#a5dd58c219d3ac12735f200e442b9ccb0">index_owner</a> (int <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#aa5036b153ca256f285b70638805fd6f3">size</a>, std::size_t index, std::size_t N)</td></tr>
<tr class="memdesc:a5dd58c219d3ac12735f200e442b9ccb0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return which rank owns index in global range [0, N - 1] (inverse of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#acb5b18fba64080a1c968fb16f556def0" title="Return local range for the calling process, partitioning the global [0, N - 1] range across all ranks...">MPI::local_range</a>).  <br /></td></tr>
<tr class="separator:a5dd58c219d3ac12735f200e442b9ccb0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a982ec85d3630a19909c3fbdfd1873641" id="r_a982ec85d3630a19909c3fbdfd1873641"><td class="memItemLeft" align="right" valign="top">std::vector&lt; int &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#a982ec85d3630a19909c3fbdfd1873641">compute_graph_edges_pcx</a> (MPI_Comm comm, std::span&lt; const int &gt; edges)</td></tr>
<tr class="memdesc:a982ec85d3630a19909c3fbdfd1873641"><td class="mdescLeft">&#160;</td><td class="mdescRight">Determine incoming graph edges using the PCX consensus algorithm.  <br /></td></tr>
<tr class="separator:a982ec85d3630a19909c3fbdfd1873641"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5b09f29e5fab78d02fa7da69b59debed" id="r_a5b09f29e5fab78d02fa7da69b59debed"><td class="memItemLeft" align="right" valign="top">std::vector&lt; int &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#a5b09f29e5fab78d02fa7da69b59debed">compute_graph_edges_nbx</a> (MPI_Comm comm, std::span&lt; const int &gt; edges)</td></tr>
<tr class="memdesc:a5b09f29e5fab78d02fa7da69b59debed"><td class="mdescLeft">&#160;</td><td class="mdescRight">Determine incoming graph edges using the NBX consensus algorithm.  <br /></td></tr>
<tr class="separator:a5b09f29e5fab78d02fa7da69b59debed"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2b53e6f2df7ead0416721b08b5b57b37" id="r_a2b53e6f2df7ead0416721b08b5b57b37"><td class="memTemplParams" colspan="2">template&lt;typename U &gt; </td></tr>
<tr class="memitem:a2b53e6f2df7ead0416721b08b5b57b37"><td class="memTemplItemLeft" align="right" valign="top">std::pair&lt; std::vector&lt; std::int32_t &gt;, std::vector&lt; typename std::remove_reference_t&lt; typename U::value_type &gt; &gt; &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#a2b53e6f2df7ead0416721b08b5b57b37">distribute_to_postoffice</a> (MPI_Comm comm, const U &amp;x, std::array&lt; std::int64_t, 2 &gt; shape, std::int64_t rank_offset)</td></tr>
<tr class="memdesc:a2b53e6f2df7ead0416721b08b5b57b37"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distribute row data to 'post office' ranks.  <br /></td></tr>
<tr class="separator:a2b53e6f2df7ead0416721b08b5b57b37"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8ef946b9df5a84ba1c55fe381f74804c" id="r_a8ef946b9df5a84ba1c55fe381f74804c"><td class="memTemplParams" colspan="2">template&lt;typename U &gt; </td></tr>
<tr class="memitem:a8ef946b9df5a84ba1c55fe381f74804c"><td class="memTemplItemLeft" align="right" valign="top">std::vector&lt; typename std::remove_reference_t&lt; typename U::value_type &gt; &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#a8ef946b9df5a84ba1c55fe381f74804c">distribute_from_postoffice</a> (MPI_Comm comm, std::span&lt; const std::int64_t &gt; indices, const U &amp;x, std::array&lt; std::int64_t, 2 &gt; shape, std::int64_t rank_offset)</td></tr>
<tr class="memdesc:a8ef946b9df5a84ba1c55fe381f74804c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distribute rows of a rectangular data array from post office ranks to ranks where they are required.  <br /></td></tr>
<tr class="separator:a8ef946b9df5a84ba1c55fe381f74804c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1690b4ee2fd6b4d6d87618788e3b2588" id="r_a1690b4ee2fd6b4d6d87618788e3b2588"><td class="memTemplParams" colspan="2">template&lt;typename U &gt; </td></tr>
<tr class="memitem:a1690b4ee2fd6b4d6d87618788e3b2588"><td class="memTemplItemLeft" align="right" valign="top">std::vector&lt; typename std::remove_reference_t&lt; typename U::value_type &gt; &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#a1690b4ee2fd6b4d6d87618788e3b2588">distribute_data</a> (MPI_Comm comm, std::span&lt; const std::int64_t &gt; indices, const U &amp;x, int shape1)</td></tr>
<tr class="memdesc:a1690b4ee2fd6b4d6d87618788e3b2588"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distribute rows of a rectangular data array to ranks where they are required (scalable version).  <br /></td></tr>
<tr class="separator:a1690b4ee2fd6b4d6d87618788e3b2588"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af4eec400c48914f3beea80ee13127aeb" id="r_af4eec400c48914f3beea80ee13127aeb"><td class="memTemplParams" colspan="2"><a id="af4eec400c48914f3beea80ee13127aeb" name="af4eec400c48914f3beea80ee13127aeb"></a>
template&lt;typename T &gt; </td></tr>
<tr class="memitem:af4eec400c48914f3beea80ee13127aeb"><td class="memTemplItemLeft" align="right" valign="top">constexpr MPI_Datatype&#160;</td><td class="memTemplItemRight" valign="bottom"><b>mpi_type</b> ()</td></tr>
<tr class="memdesc:af4eec400c48914f3beea80ee13127aeb"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> Type. <br /></td></tr>
<tr class="separator:af4eec400c48914f3beea80ee13127aeb"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> support functionality. </p>
</div><h2 class="groupheader">Function Documentation</h2>
<a id="a9cc32591cea34ccd303e379a6d694c6e" name="a9cc32591cea34ccd303e379a6d694c6e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9cc32591cea34ccd303e379a6d694c6e">&#9670;&#160;</a></span>check_error()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void check_error </td>
          <td>(</td>
          <td class="paramtype">MPI_Comm&#160;</td>
          <td class="paramname"><em>comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>code</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Check <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> error code. If the error code is not equal to MPI_SUCCESS, then std::abort is called. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">comm</td><td><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communicator </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">code</td><td>Error code returned by an <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> function call </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a5b09f29e5fab78d02fa7da69b59debed" name="a5b09f29e5fab78d02fa7da69b59debed"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5b09f29e5fab78d02fa7da69b59debed">&#9670;&#160;</a></span>compute_graph_edges_nbx()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; int &gt; compute_graph_edges_nbx </td>
          <td>(</td>
          <td class="paramtype">MPI_Comm&#160;</td>
          <td class="paramname"><em>comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::span&lt; const int &gt;&#160;</td>
          <td class="paramname"><em>edges</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Determine incoming graph edges using the NBX consensus algorithm. </p>
<p>Given a list of outgoing edges (destination ranks) from this rank, this function returns the incoming edges (source ranks) to this rank.</p>
<dl class="section note"><dt>Note</dt><dd>This function is for sparse communication patterns, i.e. where the number of ranks that communicate with each other is relatively small. It is scalable, i.e. no arrays the size of the communicator are constructed and the communication pattern is sparse. It implements the NBX algorithm presented in <a href="https://dx.doi.org/10.1145/1837853.1693476">https://dx.doi.org/10.1145/1837853.1693476</a>.</dd>
<dd>
For sparse graphs, this function has \(O(\log p)\) cost, where \(p\)is the number of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> ranks. It is suitable for modest <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> rank counts.</dd>
<dd>
The order of the returned ranks is not deterministic.</dd>
<dd>
Collective.</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">comm</td><td><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communicator </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">edges</td><td>Edges (ranks) from this rank (the caller). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Ranks that have defined edges from them to this rank. </dd></dl>

</div>
</div>
<a id="a982ec85d3630a19909c3fbdfd1873641" name="a982ec85d3630a19909c3fbdfd1873641"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a982ec85d3630a19909c3fbdfd1873641">&#9670;&#160;</a></span>compute_graph_edges_pcx()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; int &gt; compute_graph_edges_pcx </td>
          <td>(</td>
          <td class="paramtype">MPI_Comm&#160;</td>
          <td class="paramname"><em>comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::span&lt; const int &gt;&#160;</td>
          <td class="paramname"><em>edges</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Determine incoming graph edges using the PCX consensus algorithm. </p>
<p>Given a list of outgoing edges (destination ranks) from this rank, this function returns the incoming edges (source ranks) to this rank.</p>
<dl class="section note"><dt>Note</dt><dd>This function is for sparse communication patterns, i.e. where the number of ranks that communicate with each other is relatively small. It <b>is not</b> scalable as arrays the size of the communicator are allocated. It implements the PCX algorithm described in <a href="https://dx.doi.org/10.1145/1837853.1693476">https://dx.doi.org/10.1145/1837853.1693476</a>.</dd>
<dd>
For sparse graphs, this function has \(O(p)\) cost, where \(p\)is the number of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> ranks. It is suitable for modest <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> rank counts.</dd>
<dd>
The order of the returned ranks is not deterministic.</dd>
<dd>
Collective</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">comm</td><td><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communicator </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">edges</td><td>Edges (ranks) from this rank (the caller). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Ranks that have defined edges from them to this rank. </dd></dl>

</div>
</div>
<a id="a1690b4ee2fd6b4d6d87618788e3b2588" name="a1690b4ee2fd6b4d6d87618788e3b2588"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1690b4ee2fd6b4d6d87618788e3b2588">&#9670;&#160;</a></span>distribute_data()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename U &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; typename std::remove_reference_t&lt; typename U::value_type &gt; &gt; distribute_data </td>
          <td>(</td>
          <td class="paramtype">MPI_Comm&#160;</td>
          <td class="paramname"><em>comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::span&lt; const std::int64_t &gt;&#160;</td>
          <td class="paramname"><em>indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const U &amp;&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>shape1</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Distribute rows of a rectangular data array to ranks where they are required (scalable version). </p>
<p>This function determines local neighborhoods for communication, and then using <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> neighbourhood collectives to exchange data. It is scalable if the neighborhoods are relatively small, i.e. each process communicated with a modest number of other processes.</p>
<dl class="section note"><dt>Note</dt><dd>The non-scalable version of this function, MPI::distribute_data1, can be faster up to some number of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> ranks with number of ranks depending on the locality of the data, the <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> implementation and the network.</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">comm</td><td>The <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communicator </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">indices</td><td>Global indices of the data (row indices) required by calling process </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">x</td><td>Data (2D array, row-major) on calling process which may be distributed (by row). The global index for the <code>[0, ..., n)</code> local rows is assumed to be the local index plus the offset for this rank. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">shape1</td><td>The number of columns of the data array <code>x</code>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The data for each index in <code>indices</code> (row-major storage) </dd></dl>
<dl class="section pre"><dt>Precondition</dt><dd><code>shape1 &gt; 0</code> </dd></dl>

</div>
</div>
<a id="a8ef946b9df5a84ba1c55fe381f74804c" name="a8ef946b9df5a84ba1c55fe381f74804c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8ef946b9df5a84ba1c55fe381f74804c">&#9670;&#160;</a></span>distribute_from_postoffice()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename U &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; typename std::remove_reference_t&lt; typename U::value_type &gt; &gt; distribute_from_postoffice </td>
          <td>(</td>
          <td class="paramtype">MPI_Comm&#160;</td>
          <td class="paramname"><em>comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::span&lt; const std::int64_t &gt;&#160;</td>
          <td class="paramname"><em>indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const U &amp;&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::array&lt; std::int64_t, 2 &gt;&#160;</td>
          <td class="paramname"><em>shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::int64_t&#160;</td>
          <td class="paramname"><em>rank_offset</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Distribute rows of a rectangular data array from post office ranks to ranks where they are required. </p>
<p>This function determines local neighborhoods for communication, and then using <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> neighbourhood collectives to exchange data. It is scalable if the neighborhoods are relatively small, i.e. each process communicated with a modest number of other processes</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">comm</td><td>The <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communicator </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">indices</td><td>Global indices of the data (row indices) required by calling process </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">x</td><td>Data (2D array, row-major) on calling process which may be distributed (by row). The global index for the <code>[0, ..., n)</code> local rows is assumed to be the local index plus the offset for this rank. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">shape</td><td>The global shape of <code>x</code> </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rank_offset</td><td>The rank offset such that global index of local row <code>i</code> in <code>x</code> is <code>rank_offset + i</code>. It is usually computed using <code>MPI_Exscan</code>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The data for each index in <code>indices</code> (row-major storage) </dd></dl>
<dl class="section pre"><dt>Precondition</dt><dd><code>shape1 &gt; 0</code> </dd></dl>

</div>
</div>
<a id="a2b53e6f2df7ead0416721b08b5b57b37" name="a2b53e6f2df7ead0416721b08b5b57b37"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2b53e6f2df7ead0416721b08b5b57b37">&#9670;&#160;</a></span>distribute_to_postoffice()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename U &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">std::pair&lt; std::vector&lt; std::int32_t &gt;, std::vector&lt; typename std::remove_reference_t&lt; typename U::value_type &gt; &gt; &gt; distribute_to_postoffice </td>
          <td>(</td>
          <td class="paramtype">MPI_Comm&#160;</td>
          <td class="paramname"><em>comm</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const U &amp;&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::array&lt; std::int64_t, 2 &gt;&#160;</td>
          <td class="paramname"><em>shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::int64_t&#160;</td>
          <td class="paramname"><em>rank_offset</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Distribute row data to 'post office' ranks. </p>
<p>This function takes row-wise data that is distributed across processes. Data is not duplicated across ranks. The global index of a row is its local row position plus the offset for the calling process. The post office rank for a row is determined by applying <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#a5dd58c219d3ac12735f200e442b9ccb0" title="Return which rank owns index in global range [0, N - 1] (inverse of MPI::local_range).">MPI::index_owner</a> to the global index, and the row is then sent to the post office rank. The function returns that row data for which the caller is the post office.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">comm</td><td><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> communicator </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">x</td><td>Data to distribute (2D, row-major layout) </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">shape</td><td>The global shape of <code>x</code> </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rank_offset</td><td>The rank offset such that global index of local row <code>i</code> in <code>x</code> is <code>rank_offset + i</code>. It is usually computed using <code>MPI_Exscan</code>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>(0) local indices of my post office data and (1) the data (row-major). It <b>does not</b> include rows that are in <code>x</code>, i.e. rows for which the calling process is the post office </dd></dl>

</div>
</div>
<a id="a5dd58c219d3ac12735f200e442b9ccb0" name="a5dd58c219d3ac12735f200e442b9ccb0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5dd58c219d3ac12735f200e442b9ccb0">&#9670;&#160;</a></span>index_owner()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr int index_owner </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::size_t&#160;</td>
          <td class="paramname"><em>index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::size_t&#160;</td>
          <td class="paramname"><em>N</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return which rank owns index in global range [0, N - 1] (inverse of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html#acb5b18fba64080a1c968fb16f556def0" title="Return local range for the calling process, partitioning the global [0, N - 1] range across all ranks...">MPI::local_range</a>). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">size</td><td>Number of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> ranks </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">index</td><td>The index to determine owning rank </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">N</td><td>Total number of indices </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The rank of the owning process </dd></dl>

</div>
</div>
<a id="acb5b18fba64080a1c968fb16f556def0" name="acb5b18fba64080a1c968fb16f556def0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acb5b18fba64080a1c968fb16f556def0">&#9670;&#160;</a></span>local_range()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr std::array&lt; std::int64_t, 2 &gt; local_range </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>rank</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::int64_t&#160;</td>
          <td class="paramname"><em>N</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">constexpr</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return local range for the calling process, partitioning the global [0, N - 1] range across all ranks into partitions of almost equal size. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">rank</td><td><a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> rank of the caller </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">N</td><td>The value to partition </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">size</td><td>The number of <a class="el" href="../../da/dfe/namespacedolfinx_1_1MPI.html" title="MPI support functionality.">MPI</a> ranks across which to partition <code>N</code> </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
